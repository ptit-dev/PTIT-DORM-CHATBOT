import os
import sys
import shutil
import traceback 
from dotenv import load_dotenv

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from typing import Optional, Tuple
from langchain_core.language_models.llms import LLM 
from google.genai.errors import APIError as GoogleAPIError

# Ch·ªØa l·ªói Unicode tr√™n Windows
if sys.stdout.encoding.lower() != 'utf-8':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except AttributeError:
        pass

# C·∫§U H√åNH API
load_dotenv()
os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY') 
VECTOR_DB_PATH = "rag_chroma_db"

# LLM_MODEL_ID = "gemini-2.5-flash" 
LLM_MODEL_ID = "gemini-2.5-flash-lite"

# EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large" 
# EMBEDDING_MODEL_NAME = "vinai/phobert-base"
EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"

API_TIMEOUT_SECONDS = 60
RETRIEVAL_K_CHUNKS = 5

def load_llm_and_db() -> Tuple[Optional[LLM], Optional[Chroma]]:
    """
    T·∫£i LLM (Cloud API) v√† Vector Database ƒë√£ t·∫°o.
    """
    print("--- 1. KH·ªûI T·∫†O M√î H√åNH LLM (GOOGLE GEMINI API) ---")
    try:
        if not os.getenv('GOOGLE_API_KEY'):
            print("üî¥ L·ªñI: GOOGLE_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p trong file .env")
            return None, None
        
        llm = GoogleGenerativeAI(
            model=LLM_MODEL_ID,  
            temperature=0.2, 
            max_output_tokens=3069,
        )

        print(f"‚úÖ Kh·ªüi t·∫°o LLM {LLM_MODEL_ID} (Gemini API) th√†nh c√¥ng. Timeout: {API_TIMEOUT_SECONDS}s")
    except GoogleAPIError as e:
        print(f"üî¥ L·ªñI KH·ªûI T·∫†O LLM (API ERROR): {e}")
        print("Vui l√≤ng ki·ªÉm tra t√¨nh tr·∫°ng GOOGLE_API_KEY (ƒë√£ h·∫øt h·∫°n ho·∫∑c b·ªã ch·∫∑n).")
        return None, None
    except Exception as e:
        print(f"üî¥ L·ªñI KH·ªûI T·∫†O LLM: {e}")
        return None, None

    print("\n--- 2. T·∫¢I VECTOR DATABASE ---")
    if not os.path.exists(VECTOR_DB_PATH):
        print(f"üî¥ L·ªñI: Th∆∞ m·ª•c database '{VECTOR_DB_PATH}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ch·∫°y ingestion.")
        return llm, None

    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    vectorstore = Chroma(
        persist_directory=VECTOR_DB_PATH,
        embedding_function=embeddings
    )
    print("‚úÖ T·∫£i Vector Database th√†nh c√¥ng.")

    return llm, vectorstore

def generate_response(llm: LLM, vectorstore: Chroma, question: str):
    """
    Th·ª±c hi·ªán lu·ªìng RAG: Retrieval (Truy v·∫•n) -> Augmentation (TƒÉng c∆∞·ªùng) -> Generation (T·∫°o c√¢u tr·∫£ l·ªùi).
    """
    print(f"\n--- 3. TH·ª∞C HI·ªÜN TRUY V·∫§N RAG cho c√¢u h·ªèi: {question} ---")
    
    # Retrieval: L·∫§Y RETRIEVAL_K_CHUNKS (5 chunks)
    retriever = vectorstore.as_retriever(search_kwargs={"k": RETRIEVAL_K_CHUNKS}) 
    retrieved_docs = retriever.invoke(question)
    
    # Augmentation: H·ª£p nh·∫•t ng·ªØ c·∫£nh (lo·∫°i b·ªè xu·ªëng d√≤ng th·ª´a)
    context_text = "\n".join([" ".join(doc.page_content.split()) for doc in retrieved_docs])

    template = (
        "B·∫°n l√† **Chatbot H·ªó tr·ª£ Th√¥ng tin K√Ω t√∫c x√° PTIT**. Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p c√¢u tr·∫£ l·ªùi **tr·ª±c ti·∫øp, ng·∫Øn g·ªçn v√† h·ªØu √≠ch** cho sinh vi√™n.\n\n"

        "QUY T·∫ÆC B·∫ÆT BU·ªòC:\n"
        "1. **Ch·ªâ tr·∫£ l·ªùi** d·ª±a tr√™n th√¥ng tin c√≥ trong ph·∫ßn 'NG·ªÆ C·∫¢NH'. KH√îNG t·ª± suy lu·∫≠n, b·ªãa ƒë·∫∑t hay th√™m th√¥ng tin ngo√†i ng·ªØ c·∫£nh.\n"
        "2. **Gi·ªçng ƒëi·ªáu:** Th√¢n thi·ªán, d·ªÖ th∆∞∆°ng, ƒë·∫ßy ƒë·ªß x∆∞ng h√¥, chuy√™n nghi·ªáp v√† r√µ r√†ng.\n"
        "3. **C·∫•u tr√∫c tr·∫£ l·ªùi:** ƒêi th·∫≥ng v√†o c√¢u h·ªèi, tr√°nh d√πng c√°c c·ª•m t·ª´ m·ªü ƒë·∫ßu nh∆∞ 'Theo ng·ªØ c·∫£nh...', 'D∆∞·ªõi ƒë√¢y l√† th√¥ng tin t√¥i t√¨m th·∫•y...'.\n"
        "4. **X·ª≠ l√Ω thi·∫øu th√¥ng tin:** N·∫øu 'NG·ªÆ C·∫¢NH' KH√îNG C√ì th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi, Tr·∫£ l·ªùi ƒë·∫°i √Ω ki·ªÉu: 'Xin l·ªói, M√¨nh ƒë√£ ki·ªÉm tra nh∆∞ng ch∆∞a th·∫•y th√¥ng tin v·ªÅ n·ªôi dung n√†y. B·∫°n vui l√≤ng li√™n h·ªá Ban Qu·∫£n l√Ω KTX ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ th√™m nh√©.'"
        "5. Tr·∫£ l·ªùi th·∫≠t ƒë·∫ßy ƒë·ªß th√¥ng tin \n\n"

        "NG·ªÆ C·∫¢NH:\n"
        "--- B·ªëi c·∫£nh d·ªØ li·ªáu hi·ªán t·∫°i (Ng√†y {current_date}) ---\n"
        "{context}\n"
        "--- K·∫æT TH√öC NG·ªÆ C·∫¢NH ---\n\n"

        "C√¢u h·ªèi c·ªßa sinh vi√™n:\n"
        "{question}\n\n"

        "H√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi **tr·ª±c ti·∫øp**:"
    )
    
    rag_prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question", "current_date"]
    )

    from datetime import datetime
    current_date = datetime.now().strftime("%d/%m/%Y")
    final_prompt = rag_prompt.format(context=context_text, question=question, current_date=current_date)

    print(f"-> Prompt (ƒë·∫ßu v√†o LLM):\n{final_prompt}")
    print(f"-> B·∫ÆT ƒê·∫¶U g·ªçi API ƒë·∫øn LLM ({LLM_MODEL_ID}) qua Cloud API...")
    
    try:
        response = llm.invoke(final_prompt)
        print("-> K·∫æT TH√öC g·ªçi API th√†nh c√¥ng.")
        
        # KI·ªÇM TRA ƒê·∫¶U RA R·ªñNG (EMPTY STRING CHECK)
        if not response or response.strip() == "":
            return "Xin l·ªói, m√¥ h√¨nh LLM ƒë√£ kh√¥ng th·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi h·ª£p l·ªá d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c thay ƒë·ªïi c√¢u h·ªèi."
            
        return response
    except Exception as e:
        # X·ª≠ l√Ω l·ªói API (n·∫øu c√≥)
        traceback.print_exc()
        raise Exception(f"L·ªói khi g·ªçi API Gemini: {str(e)}")

